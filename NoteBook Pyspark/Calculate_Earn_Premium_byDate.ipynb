{"cells":[{"cell_type":"code","execution_count":7,"id":"ca7dfa79-9be6-42bd-86e7-eb82f4180262","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2025-04-21T02:54:56.4030711Z","execution_start_time":"2025-04-21T02:54:39.4194808Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"2e54dfd4-6cb1-455d-b2f6-ff56aef98dda","queued_time":"2025-04-21T02:54:39.4175299Z","session_id":"760acdad-592c-47d2-81a6-920f0c22a58b","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":9,"statement_ids":[9]},"text/plain":["StatementMeta(, 760acdad-592c-47d2-81a6-920f0c22a58b, 9, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DataFrame[std_v_date_issue: date, std_v_date_expiry: date]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Script này dùng để tính toán tỷ lệ phân bổ phí bảo hiểm (earned rate và UPR rate) theo từng ngày \n","# giữa ngày cấp đơn và ngày hết hạn, sau đó ghi kết quả vào bảng lưu trữ Delta Table.\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, lit, datediff, expr, current_timestamp, explode, sequence, to_date, when, year, coalesce\n","from pyspark.sql.window import Window\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import broadcast\n","\n","def get_spark_session():\n","    spark = SparkSession.builder \\\n","        .appName(\"MotorDataEarnedCalculate\") \\\n","        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n","        .config(\"spark.default.parallelism\", \"200\") \\\n","        .getOrCreate()\n","    return spark\n","\n","spark = get_spark_session()\n","\n","try:\n","    raw_data = spark.read.format(\"delta\").table(\"raw_motor_data\").cache()\n","except Exception as e:\n","    print(f\"Error reading raw_motor_data: {str(e)}\")\n","    raise Exception(\"Please ensure raw_motor_data exists in the Lakehouse.\")\n","\n","std_data = raw_data.withColumn(\n","    \"P_DATE_ISSUE\", col(\"std_v_date_issue\")\n",").withColumn(\n","    \"P_DATE_EXPIRY\",\n","    when(\n","        (col(\"std_v_date_expiry\") <= col(\"std_v_date_issue\")) |\n","        (datediff(col(\"std_v_date_expiry\"), col(\"std_v_date_issue\")) / 365 > 15),\n","        F.date_add(col(\"std_v_date_issue\"), 1)\n","    ).otherwise(col(\"std_v_date_expiry\"))\n",")\n","\n","std_days = std_data.withColumn(\n","    \"DAY_SPLIT\",\n","    explode(\n","        sequence(\n","            to_date(col(\"P_DATE_ISSUE\")), \n","            to_date(col(\"P_DATE_EXPIRY\")), \n","            expr(\"interval 1 day\")\n","        )\n","    )\n",")\n","\n","std_final = std_days.withColumn(\n","    \"PERCENT_PREMIUM\",\n","    (1.0 / datediff(col(\"P_DATE_EXPIRY\"), col(\"P_DATE_ISSUE\")))\n",")\n","\n","window_spec = Window.partitionBy(\"P_DATE_ISSUE\", \"P_DATE_EXPIRY\").orderBy(\"DAY_SPLIT\")\n","\n","std_final = std_final.withColumn(\n","    \"CUMULATIVE_PERCENT_PREMIUM\",\n","    F.sum(\"PERCENT_PREMIUM\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",").withColumn(\n","    \"PERCENT_PREMIUM_ADJ\",\n","    when(\n","        col(\"DAY_SPLIT\") == col(\"P_DATE_EXPIRY\"),\n","        1.0 - F.sum(\"PERCENT_PREMIUM\").over(window_spec.rowsBetween(Window.unboundedPreceding, -1))\n","    ).otherwise(\n","        col(\"PERCENT_PREMIUM\")\n","    )\n",").withColumn(\n","    \"PERCENT_PREMIUM_ADJ\",\n","    when(col(\"PERCENT_PREMIUM_ADJ\") < 0, lit(0.0)).otherwise(col(\"PERCENT_PREMIUM_ADJ\"))\n",")\n","\n","std_final = std_final.withColumn(\n","    \"UPR_RATE\",\n","    when(\n","        col(\"DAY_SPLIT\") == col(\"P_DATE_EXPIRY\"),\n","        lit(0.0)\n","    ).otherwise(\n","        F.greatest(\n","            lit(0.0),\n","            1.0 - F.sum(\"PERCENT_PREMIUM_ADJ\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n","        )\n","    )\n",").withColumn(\n","    \"UPR_RATE\",\n","    when(col(\"DAY_SPLIT\") == col(\"P_DATE_EXPIRY\"), lit(0.0)).otherwise(col(\"UPR_RATE\"))\n",")\n","\n","final_data = std_final.select(\n","    current_timestamp().cast('date').alias(\"date_data\"),\n","    col(\"std_v_date_issue\"),\n","    col(\"std_v_date_expiry\"),\n","    col(\"DAY_SPLIT\").cast(\"date\").alias(\"earn_date\"),\n","    col(\"PERCENT_PREMIUM_ADJ\").cast(\"double\").alias(\"EARNED_RATE\"),\n","    col(\"UPR_RATE\").cast(\"double\").alias(\"UPR_RATE\")\n",")\n","\n","final_data.repartition(200) \\\n","    .write \\\n","    .format(\"delta\") \\\n","    .mode(\"overwrite\") \\\n","    .saveAsTable(\"A6_MOTOR_DATE_EARNED_RATE\")\n","\n","raw_data.unpersist()"]}],"metadata":{"dependencies":{"environment":{},"lakehouse":{"default_lakehouse":"278886d5-bad9-4eac-a439-496a1da7398e","default_lakehouse_name":"Earn_Motor_LakeHouse","default_lakehouse_workspace_id":"30dba040-2679-4d12-8ab3-0d871b8f2bb0","known_lakehouses":[{"id":"278886d5-bad9-4eac-a439-496a1da7398e"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"state":{"2090705f-43eb-4991-941d-a7589c94eb4d":{"persist_state":{"view":{"chartOptions":{"aggregationType":"sum","binsNumber":10,"categoryFieldKeys":[],"chartType":"bar","evaluatesOverAllRecords":false,"isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details","viewOptionsGroup":[{"tabItems":[{"key":"0","name":"Table","options":{},"type":"table"}]}]}},"sync_state":{"isSummary":false,"language":"scala","table":{"rows":[{"0":"1.3752042345410002E10"}],"schema":[{"key":"0","name":"sum(TOTAL_CLAIM)","type":"double"}],"truncated":false},"wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"type":"Synapse.DataFrame"}},"version":"0.1"}},"nbformat":4,"nbformat_minor":5}
